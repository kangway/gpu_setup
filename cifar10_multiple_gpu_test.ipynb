{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kangway/cifar10\n"
     ]
    }
   ],
   "source": [
    "%cd /home/kangway/cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 x NVIDIA GEFORCE 1080Ti GPUs, set num_gpus=2 under flags in script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "2017-05-11 22:15:06.846400: step 0, loss = 4.68 (31.7 examples/sec; 4.040 sec/batch)\n",
      "2017-05-11 22:15:07.521177: step 10, loss = 4.63 (4433.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:08.081749: step 20, loss = 4.48 (4594.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:08.649922: step 30, loss = 4.35 (4397.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:09.222496: step 40, loss = 4.39 (4073.3 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:15:09.796777: step 50, loss = 4.28 (4651.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:10.366784: step 60, loss = 4.14 (4360.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:10.934858: step 70, loss = 4.11 (4366.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:11.507624: step 80, loss = 4.15 (4529.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:12.069785: step 90, loss = 4.14 (4848.4 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:15:12.643443: step 100, loss = 4.14 (4201.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:15:13.250953: step 110, loss = 4.13 (5223.3 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:15:13.835242: step 120, loss = 3.94 (4017.4 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:15:14.408701: step 130, loss = 3.94 (4558.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:14.978602: step 140, loss = 3.98 (4557.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:15.546154: step 150, loss = 3.89 (4303.6 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:15:16.102213: step 160, loss = 4.09 (5129.6 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:15:16.676782: step 170, loss = 3.94 (4717.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:15:17.236963: step 180, loss = 4.12 (4237.9 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:15:17.798990: step 190, loss = 3.99 (4598.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:18.359798: step 200, loss = 3.81 (4705.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:15:18.980997: step 210, loss = 3.92 (4972.5 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:15:19.546329: step 220, loss = 3.80 (4333.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:15:20.108223: step 230, loss = 3.67 (4581.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:20.675951: step 240, loss = 3.88 (4309.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:15:21.242515: step 250, loss = 3.81 (4565.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:21.802657: step 260, loss = 3.59 (4713.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:15:22.377373: step 270, loss = 3.55 (4445.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:22.945215: step 280, loss = 3.64 (4816.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:15:23.518838: step 290, loss = 3.65 (4420.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:24.086947: step 300, loss = 3.64 (5120.1 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:15:24.737195: step 310, loss = 3.39 (4611.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:25.314661: step 320, loss = 3.47 (4435.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:25.876510: step 330, loss = 3.56 (4429.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:26.444991: step 340, loss = 3.50 (4619.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:27.020726: step 350, loss = 3.44 (4624.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:27.596922: step 360, loss = 3.40 (4680.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:15:28.191287: step 370, loss = 3.40 (4470.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:28.762502: step 380, loss = 3.42 (4385.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:29.338539: step 390, loss = 3.33 (4410.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:29.909470: step 400, loss = 3.30 (4441.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:30.511743: step 410, loss = 3.36 (4135.4 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:15:31.085607: step 420, loss = 3.34 (4323.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:15:31.654959: step 430, loss = 3.33 (4751.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:15:32.221376: step 440, loss = 3.24 (4709.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:15:32.786612: step 450, loss = 3.06 (4540.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:33.361830: step 460, loss = 3.26 (4171.1 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:15:33.930485: step 470, loss = 3.26 (4461.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:34.500538: step 480, loss = 3.23 (4013.1 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:15:35.071390: step 490, loss = 3.14 (3857.1 examples/sec; 0.033 sec/batch)\n",
      "2017-05-11 22:15:35.628791: step 500, loss = 3.22 (4849.5 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:15:36.245526: step 510, loss = 3.28 (4497.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:36.814729: step 520, loss = 3.10 (4507.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:37.391581: step 530, loss = 3.10 (4399.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:37.948011: step 540, loss = 3.41 (4311.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:15:38.514806: step 550, loss = 2.93 (4390.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:39.082752: step 560, loss = 3.20 (4906.7 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:15:39.662443: step 570, loss = 2.93 (4243.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:15:40.226646: step 580, loss = 3.09 (4404.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:40.797850: step 590, loss = 2.90 (4304.2 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:15:41.356269: step 600, loss = 2.89 (4847.8 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:15:41.992122: step 610, loss = 2.78 (4755.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:15:42.569914: step 620, loss = 3.07 (4469.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:43.134237: step 630, loss = 2.86 (4452.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:43.699337: step 640, loss = 3.03 (4626.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:44.266495: step 650, loss = 2.95 (4485.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:44.837614: step 660, loss = 2.81 (4519.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:45.416727: step 670, loss = 2.84 (4500.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:45.979570: step 680, loss = 2.77 (4450.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:46.543300: step 690, loss = 2.80 (4413.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:47.114437: step 700, loss = 2.65 (4381.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:47.740809: step 710, loss = 2.63 (4984.5 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:15:48.305795: step 720, loss = 2.77 (4271.3 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:15:48.867127: step 730, loss = 2.65 (4504.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:49.434124: step 740, loss = 2.75 (4370.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:50.011698: step 750, loss = 2.56 (4530.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:50.593582: step 760, loss = 2.79 (5157.0 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:15:51.162540: step 770, loss = 2.70 (4279.0 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:15:51.727475: step 780, loss = 2.72 (4305.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:15:52.302530: step 790, loss = 2.76 (4428.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:52.877243: step 800, loss = 2.76 (4995.0 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:15:53.481965: step 810, loss = 2.62 (4685.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:15:54.061822: step 820, loss = 2.65 (4521.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:54.634753: step 830, loss = 2.79 (4354.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:55.198580: step 840, loss = 2.59 (4700.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:15:55.761805: step 850, loss = 2.59 (4686.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:15:56.334785: step 860, loss = 2.46 (4535.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:56.899018: step 870, loss = 2.55 (4364.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:57.473464: step 880, loss = 2.66 (4572.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:58.043531: step 890, loss = 2.56 (4568.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:15:58.612257: step 900, loss = 2.67 (4362.6 examples/sec; 0.029 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-11 22:15:59.232644: step 910, loss = 2.44 (4453.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:15:59.805602: step 920, loss = 2.43 (4771.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:00.374070: step 930, loss = 2.73 (4351.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:00.939163: step 940, loss = 2.54 (4761.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:01.529063: step 950, loss = 2.27 (3918.4 examples/sec; 0.033 sec/batch)\n",
      "2017-05-11 22:16:02.100565: step 960, loss = 2.38 (4397.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:02.673483: step 970, loss = 2.48 (4790.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:03.252098: step 980, loss = 2.19 (4081.3 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:16:03.828570: step 990, loss = 2.72 (4168.1 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:16:04.404508: step 1000, loss = 2.53 (4629.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:05.079745: step 1010, loss = 2.37 (4377.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:05.645253: step 1020, loss = 2.36 (4663.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:06.220953: step 1030, loss = 2.26 (4442.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:06.793519: step 1040, loss = 2.80 (4449.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:07.369672: step 1050, loss = 2.30 (5044.8 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:16:07.956675: step 1060, loss = 2.10 (4472.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:08.534129: step 1070, loss = 2.55 (4388.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:09.103291: step 1080, loss = 2.19 (4788.1 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:09.678683: step 1090, loss = 2.36 (4443.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:10.244772: step 1100, loss = 2.34 (4755.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:10.878587: step 1110, loss = 2.21 (4549.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:11.446481: step 1120, loss = 2.07 (4853.3 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:16:12.024214: step 1130, loss = 2.46 (4115.4 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:16:12.588309: step 1140, loss = 2.34 (4662.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:13.166825: step 1150, loss = 2.03 (4287.3 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:13.736762: step 1160, loss = 2.26 (4295.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:14.302676: step 1170, loss = 2.16 (4743.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:14.884721: step 1180, loss = 2.27 (4078.9 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:16:15.453782: step 1190, loss = 2.32 (4551.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:16.020180: step 1200, loss = 2.18 (4294.2 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:16.638405: step 1210, loss = 2.23 (4067.9 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:16:17.205663: step 1220, loss = 2.02 (4133.2 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:16:17.768354: step 1230, loss = 2.16 (5224.3 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:16:18.344615: step 1240, loss = 2.37 (4707.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:18.919424: step 1250, loss = 2.37 (4642.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:19.517692: step 1260, loss = 2.04 (4247.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:20.084954: step 1270, loss = 2.13 (4878.6 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:16:20.649827: step 1280, loss = 1.90 (4549.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:21.228231: step 1290, loss = 2.12 (4241.6 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:21.810250: step 1300, loss = 1.89 (4502.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:22.441618: step 1310, loss = 2.06 (4461.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:23.022068: step 1320, loss = 2.18 (4417.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:23.597862: step 1330, loss = 2.35 (4804.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:24.169365: step 1340, loss = 2.01 (4360.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:24.746231: step 1350, loss = 2.01 (4280.0 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:25.313626: step 1360, loss = 1.98 (4303.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:25.885320: step 1370, loss = 1.84 (4398.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:26.450521: step 1380, loss = 1.73 (4899.2 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:16:27.028557: step 1390, loss = 2.18 (4431.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:27.597169: step 1400, loss = 1.95 (4554.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:28.217046: step 1410, loss = 2.03 (4599.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:28.792308: step 1420, loss = 1.90 (4664.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:29.381308: step 1430, loss = 1.93 (4300.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:29.959615: step 1440, loss = 1.94 (4370.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:30.524711: step 1450, loss = 1.88 (4805.1 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:31.110667: step 1460, loss = 1.85 (4082.4 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:16:31.681670: step 1470, loss = 1.88 (4195.7 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:16:32.253995: step 1480, loss = 1.89 (4776.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:32.822518: step 1490, loss = 1.83 (4211.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:33.406744: step 1500, loss = 1.82 (4678.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:34.050501: step 1510, loss = 1.82 (4700.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:34.624442: step 1520, loss = 1.88 (3903.9 examples/sec; 0.033 sec/batch)\n",
      "2017-05-11 22:16:35.194513: step 1530, loss = 2.01 (4668.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:35.765148: step 1540, loss = 1.78 (4587.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:36.340890: step 1550, loss = 1.82 (4415.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:36.911472: step 1560, loss = 1.84 (4310.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:37.487256: step 1570, loss = 1.72 (4541.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:38.061742: step 1580, loss = 1.81 (4263.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:38.637895: step 1590, loss = 1.73 (4328.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:39.203740: step 1600, loss = 1.67 (4190.9 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:16:39.840730: step 1610, loss = 1.84 (4564.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:40.417599: step 1620, loss = 1.80 (4043.4 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:16:40.987724: step 1630, loss = 1.68 (4155.2 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:16:41.540888: step 1640, loss = 1.65 (4889.5 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:16:42.105237: step 1650, loss = 1.88 (4530.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:42.675419: step 1660, loss = 1.67 (4147.8 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:16:43.243633: step 1670, loss = 1.89 (4648.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:43.815277: step 1680, loss = 1.70 (4429.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:44.393884: step 1690, loss = 1.61 (4374.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:44.950301: step 1700, loss = 1.78 (4829.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:45.577509: step 1710, loss = 1.86 (4672.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:46.163189: step 1720, loss = 1.62 (4023.2 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:16:46.733354: step 1730, loss = 1.52 (4557.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:47.294643: step 1740, loss = 1.76 (4520.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:47.870026: step 1750, loss = 1.65 (4696.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:48.445080: step 1760, loss = 1.59 (4441.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:49.015787: step 1770, loss = 1.66 (4320.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:49.596432: step 1780, loss = 1.79 (3962.1 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:16:50.164994: step 1790, loss = 1.73 (4503.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:50.736937: step 1800, loss = 1.57 (4508.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:16:51.352796: step 1810, loss = 1.63 (4681.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:51.924745: step 1820, loss = 1.74 (4523.2 examples/sec; 0.028 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-11 22:16:52.492129: step 1830, loss = 1.69 (5174.5 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:16:53.078213: step 1840, loss = 1.70 (3897.9 examples/sec; 0.033 sec/batch)\n",
      "2017-05-11 22:16:53.648418: step 1850, loss = 1.44 (4157.1 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:16:54.229532: step 1860, loss = 1.99 (4104.6 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:16:54.804885: step 1870, loss = 1.83 (4817.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:55.373272: step 1880, loss = 1.65 (4280.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:55.955595: step 1890, loss = 1.50 (4910.1 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:16:56.529564: step 1900, loss = 1.58 (4291.0 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:57.152313: step 1910, loss = 1.51 (4087.5 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:16:57.735921: step 1920, loss = 1.60 (4673.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:16:58.308834: step 1930, loss = 1.43 (4265.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:16:58.877375: step 1940, loss = 1.66 (4459.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:16:59.449616: step 1950, loss = 1.39 (4773.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:00.017667: step 1960, loss = 1.55 (4533.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:00.588127: step 1970, loss = 1.69 (5176.8 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:17:01.171443: step 1980, loss = 1.73 (4413.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:01.755141: step 1990, loss = 1.62 (4374.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:02.328989: step 2000, loss = 1.66 (5264.8 examples/sec; 0.024 sec/batch)\n",
      "2017-05-11 22:17:03.032952: step 2010, loss = 1.50 (4627.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:03.608673: step 2020, loss = 1.52 (4203.9 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:17:04.179049: step 2030, loss = 1.54 (4752.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:04.758406: step 2040, loss = 1.65 (4411.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:05.330807: step 2050, loss = 1.30 (4253.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:17:05.892274: step 2060, loss = 1.55 (4299.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:17:06.457572: step 2070, loss = 1.48 (4657.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:07.026690: step 2080, loss = 1.50 (4758.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:07.599169: step 2090, loss = 1.30 (4340.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:08.163361: step 2100, loss = 1.31 (4584.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:08.771844: step 2110, loss = 1.39 (4468.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:09.350664: step 2120, loss = 1.50 (4709.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:09.921624: step 2130, loss = 1.37 (4097.3 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:17:10.502191: step 2140, loss = 1.54 (4129.3 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:17:11.082524: step 2150, loss = 1.44 (4148.4 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:17:11.653378: step 2160, loss = 1.50 (4683.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:12.226764: step 2170, loss = 1.53 (4045.7 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:17:12.811426: step 2180, loss = 1.46 (4441.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:13.389735: step 2190, loss = 1.37 (4249.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:17:13.955682: step 2200, loss = 1.55 (4423.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:14.587582: step 2210, loss = 1.69 (4304.0 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:17:15.169002: step 2220, loss = 1.46 (4689.1 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:15.740577: step 2230, loss = 1.53 (4488.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:16.314451: step 2240, loss = 1.64 (3957.6 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:17:16.875739: step 2250, loss = 1.64 (4448.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:17.445658: step 2260, loss = 1.45 (4223.6 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:17:18.011801: step 2270, loss = 1.47 (4742.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:18.580748: step 2280, loss = 1.31 (4124.6 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:17:19.152277: step 2290, loss = 1.68 (4518.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:19.720125: step 2300, loss = 1.49 (5187.4 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:17:20.358882: step 2310, loss = 1.35 (4602.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:20.939920: step 2320, loss = 1.38 (4431.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:21.501143: step 2330, loss = 1.40 (4781.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:22.076640: step 2340, loss = 1.32 (4544.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:22.657416: step 2350, loss = 1.40 (4391.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:23.231356: step 2360, loss = 1.53 (4491.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:23.803322: step 2370, loss = 1.43 (4372.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:24.378253: step 2380, loss = 1.53 (4421.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:24.946235: step 2390, loss = 1.46 (4587.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:25.517040: step 2400, loss = 1.48 (4663.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:26.174198: step 2410, loss = 1.38 (4729.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:26.752016: step 2420, loss = 1.23 (4635.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:27.337323: step 2430, loss = 1.36 (4705.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:27.924336: step 2440, loss = 1.24 (4005.5 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:17:28.502102: step 2450, loss = 1.24 (4458.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:29.077249: step 2460, loss = 1.56 (4318.2 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:17:29.641053: step 2470, loss = 1.38 (4711.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:30.209912: step 2480, loss = 1.32 (4465.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:30.769481: step 2490, loss = 1.39 (4761.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:31.342915: step 2500, loss = 1.28 (4941.9 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:17:31.993204: step 2510, loss = 1.29 (4479.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:32.557425: step 2520, loss = 1.32 (5003.5 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:17:33.128671: step 2530, loss = 1.37 (4625.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:33.689351: step 2540, loss = 1.21 (4762.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:34.257138: step 2550, loss = 1.46 (4584.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:34.834855: step 2560, loss = 1.39 (4425.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:35.404646: step 2570, loss = 1.19 (4771.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:35.974493: step 2580, loss = 1.51 (3963.3 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:17:36.554213: step 2590, loss = 1.23 (4168.7 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:17:37.124874: step 2600, loss = 1.33 (4377.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:37.778404: step 2610, loss = 1.15 (4241.0 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:17:38.342343: step 2620, loss = 1.23 (4488.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:38.914489: step 2630, loss = 1.28 (5373.1 examples/sec; 0.024 sec/batch)\n",
      "2017-05-11 22:17:39.486309: step 2640, loss = 1.34 (4888.2 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:17:40.057185: step 2650, loss = 1.29 (4713.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:40.631782: step 2660, loss = 1.58 (4564.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:41.206098: step 2670, loss = 1.15 (4185.1 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:17:41.790768: step 2680, loss = 1.20 (4529.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:42.358435: step 2690, loss = 1.32 (4664.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:42.935538: step 2700, loss = 1.41 (4678.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:43.553231: step 2710, loss = 1.23 (4332.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:17:44.130978: step 2720, loss = 1.27 (4740.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:44.704679: step 2730, loss = 1.29 (4569.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:45.274379: step 2740, loss = 1.25 (5078.2 examples/sec; 0.025 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-11 22:17:45.849003: step 2750, loss = 1.57 (4432.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:46.419106: step 2760, loss = 1.31 (4668.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:47.004313: step 2770, loss = 1.08 (3768.1 examples/sec; 0.034 sec/batch)\n",
      "2017-05-11 22:17:47.583398: step 2780, loss = 1.26 (3810.1 examples/sec; 0.034 sec/batch)\n",
      "2017-05-11 22:17:48.144452: step 2790, loss = 1.11 (4449.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:48.709741: step 2800, loss = 1.27 (4834.0 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:17:49.364214: step 2810, loss = 1.21 (4635.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:49.949714: step 2820, loss = 1.21 (4371.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:50.520806: step 2830, loss = 1.31 (4493.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:51.088865: step 2840, loss = 1.15 (4327.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:17:51.648131: step 2850, loss = 1.29 (4829.1 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:52.212448: step 2860, loss = 1.11 (4726.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:52.796265: step 2870, loss = 1.02 (4246.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:17:53.366577: step 2880, loss = 1.10 (4540.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:17:53.943989: step 2890, loss = 0.89 (4714.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:54.510980: step 2900, loss = 1.22 (4927.1 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:17:55.146431: step 2910, loss = 1.41 (4161.6 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:17:55.724996: step 2920, loss = 1.22 (4436.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:56.304023: step 2930, loss = 1.20 (4773.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:56.887739: step 2940, loss = 1.23 (4261.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:17:57.468691: step 2950, loss = 1.31 (4168.4 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:17:58.045033: step 2960, loss = 1.13 (4406.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:58.626335: step 2970, loss = 1.44 (4352.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:17:59.198140: step 2980, loss = 1.13 (4681.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:17:59.782143: step 2990, loss = 1.26 (4515.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:00.347505: step 3000, loss = 1.14 (4608.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:01.035134: step 3010, loss = 1.15 (4019.3 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:18:01.606534: step 3020, loss = 1.13 (4199.0 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:02.164388: step 3030, loss = 0.98 (4581.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:02.744396: step 3040, loss = 1.11 (4627.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:03.309640: step 3050, loss = 1.16 (4651.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:03.889424: step 3060, loss = 0.98 (4299.9 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:04.462202: step 3070, loss = 1.25 (4475.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:05.035760: step 3080, loss = 1.23 (4646.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:05.597256: step 3090, loss = 1.14 (4627.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:06.167283: step 3100, loss = 1.18 (4488.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:06.827502: step 3110, loss = 1.11 (4524.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:07.405265: step 3120, loss = 1.29 (4411.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:07.973769: step 3130, loss = 1.13 (4513.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:08.549942: step 3140, loss = 1.20 (4362.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:09.132335: step 3150, loss = 1.18 (4415.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:09.708536: step 3160, loss = 1.22 (4556.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:10.275412: step 3170, loss = 1.06 (4460.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:10.844773: step 3180, loss = 1.17 (4036.4 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:18:11.412836: step 3190, loss = 1.07 (4641.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:11.985920: step 3200, loss = 0.99 (4241.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:12.610520: step 3210, loss = 1.06 (4263.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:13.181199: step 3220, loss = 1.18 (4635.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:13.749819: step 3230, loss = 1.26 (5144.3 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:18:14.326153: step 3240, loss = 1.04 (4591.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:14.903751: step 3250, loss = 1.09 (4732.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:18:15.471811: step 3260, loss = 1.15 (4622.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:16.045017: step 3270, loss = 1.15 (4490.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:16.612650: step 3280, loss = 1.14 (4516.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:17.189015: step 3290, loss = 1.06 (4272.0 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:17.767793: step 3300, loss = 1.23 (4518.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:18.380565: step 3310, loss = 1.03 (4434.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:18.954717: step 3320, loss = 1.21 (4384.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:19.521193: step 3330, loss = 1.10 (3930.8 examples/sec; 0.033 sec/batch)\n",
      "2017-05-11 22:18:20.099504: step 3340, loss = 1.11 (4484.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:20.682482: step 3350, loss = 1.12 (4151.9 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:18:21.244718: step 3360, loss = 1.11 (4452.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:21.812262: step 3370, loss = 1.32 (4409.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:22.364756: step 3380, loss = 1.04 (4888.5 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:18:22.943579: step 3390, loss = 1.08 (4275.2 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:23.518214: step 3400, loss = 1.19 (4909.6 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:18:24.156922: step 3410, loss = 1.12 (4343.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:24.728235: step 3420, loss = 1.38 (4139.8 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:18:25.293475: step 3430, loss = 1.06 (4236.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:25.869686: step 3440, loss = 1.24 (4479.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:26.448350: step 3450, loss = 1.06 (4570.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:27.020458: step 3460, loss = 0.96 (4454.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:27.583920: step 3470, loss = 1.06 (4357.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:28.153551: step 3480, loss = 0.93 (4164.6 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:18:28.717675: step 3490, loss = 1.14 (4335.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:29.291852: step 3500, loss = 1.03 (4521.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:29.922620: step 3510, loss = 1.19 (4059.1 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:18:30.489499: step 3520, loss = 1.16 (4451.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:31.061428: step 3530, loss = 0.92 (5019.8 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:18:31.635859: step 3540, loss = 0.96 (4540.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:32.214167: step 3550, loss = 1.28 (4262.9 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:32.771602: step 3560, loss = 1.21 (4701.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:18:33.348951: step 3570, loss = 1.12 (4500.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:33.916597: step 3580, loss = 1.04 (4356.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:34.506081: step 3590, loss = 0.97 (4453.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:35.076735: step 3600, loss = 1.15 (4320.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:35.710036: step 3610, loss = 0.92 (4286.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:36.286288: step 3620, loss = 1.00 (4541.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:36.862473: step 3630, loss = 1.00 (4508.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:37.443965: step 3640, loss = 1.25 (4214.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:38.019914: step 3650, loss = 1.11 (4176.0 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:18:38.597088: step 3660, loss = 0.95 (3908.0 examples/sec; 0.033 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-11 22:18:39.167481: step 3670, loss = 1.24 (4539.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:39.744247: step 3680, loss = 1.08 (4553.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:40.320899: step 3690, loss = 0.96 (4317.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:40.902439: step 3700, loss = 1.07 (4888.0 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:18:41.548450: step 3710, loss = 1.00 (4272.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:42.129007: step 3720, loss = 1.05 (4372.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:42.696340: step 3730, loss = 0.86 (4391.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:43.277970: step 3740, loss = 1.04 (4545.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:43.850838: step 3750, loss = 0.95 (4965.5 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:18:44.417849: step 3760, loss = 0.95 (5002.7 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:18:44.983383: step 3770, loss = 1.05 (4470.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:45.559486: step 3780, loss = 1.15 (4478.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:46.132119: step 3790, loss = 1.18 (4493.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:46.694449: step 3800, loss = 1.13 (4354.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:47.313715: step 3810, loss = 1.12 (4406.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:47.903540: step 3820, loss = 0.99 (3870.7 examples/sec; 0.033 sec/batch)\n",
      "2017-05-11 22:18:48.472565: step 3830, loss = 1.00 (4584.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:49.047483: step 3840, loss = 0.92 (4600.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:49.637175: step 3850, loss = 0.99 (4389.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:50.208839: step 3860, loss = 1.11 (4828.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:18:50.776569: step 3870, loss = 0.89 (4453.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:51.353505: step 3880, loss = 1.19 (4790.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:18:51.942038: step 3890, loss = 1.03 (4084.8 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:18:52.515982: step 3900, loss = 0.92 (4620.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:53.124307: step 3910, loss = 1.07 (4459.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:53.712082: step 3920, loss = 1.03 (4427.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:54.293015: step 3930, loss = 0.89 (4597.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:18:54.863052: step 3940, loss = 1.07 (4291.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:55.437935: step 3950, loss = 0.98 (4416.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:18:55.993791: step 3960, loss = 1.02 (4760.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:18:56.571403: step 3970, loss = 1.04 (4697.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:18:57.147786: step 3980, loss = 1.11 (4301.0 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:18:57.716094: step 3990, loss = 1.03 (5085.6 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:18:58.290531: step 4000, loss = 0.95 (4165.6 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:18:58.978531: step 4010, loss = 1.14 (5012.5 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:18:59.562626: step 4020, loss = 0.91 (4147.0 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:19:00.132943: step 4030, loss = 1.08 (4355.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:00.704726: step 4040, loss = 0.90 (4553.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:01.277707: step 4050, loss = 1.00 (4199.0 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:19:01.859374: step 4060, loss = 0.98 (4128.4 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:19:02.439228: step 4070, loss = 1.27 (4313.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:19:03.012313: step 4080, loss = 0.94 (3977.8 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:19:03.579830: step 4090, loss = 0.96 (4583.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:04.166723: step 4100, loss = 1.09 (4389.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:04.786635: step 4110, loss = 1.04 (4553.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:05.354443: step 4120, loss = 0.95 (4618.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:05.931386: step 4130, loss = 0.96 (4129.1 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:19:06.487992: step 4140, loss = 0.94 (4824.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:19:07.060166: step 4150, loss = 0.95 (5013.1 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:19:07.632888: step 4160, loss = 0.96 (4078.9 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:19:08.207582: step 4170, loss = 0.98 (4468.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:08.781177: step 4180, loss = 1.06 (4373.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:09.350995: step 4190, loss = 1.07 (4490.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:09.918261: step 4200, loss = 1.24 (4585.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:10.540973: step 4210, loss = 0.88 (4425.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:11.124343: step 4220, loss = 0.93 (4447.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:11.690255: step 4230, loss = 1.01 (5523.9 examples/sec; 0.023 sec/batch)\n",
      "2017-05-11 22:19:12.270757: step 4240, loss = 1.05 (4492.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:12.846648: step 4250, loss = 1.10 (4653.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:13.425735: step 4260, loss = 0.97 (4609.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:13.995664: step 4270, loss = 1.17 (4855.6 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:19:14.560731: step 4280, loss = 0.89 (4625.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:15.138904: step 4290, loss = 0.97 (4878.1 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:19:15.714980: step 4300, loss = 1.07 (4333.3 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:19:16.336632: step 4310, loss = 0.90 (4147.0 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:19:16.911255: step 4320, loss = 1.15 (4335.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:19:17.484673: step 4330, loss = 0.87 (4753.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:19:18.054379: step 4340, loss = 1.01 (4385.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:18.621492: step 4350, loss = 1.05 (4281.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:19:19.207063: step 4360, loss = 0.96 (4629.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:19.769075: step 4370, loss = 0.92 (4753.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:19:20.348861: step 4380, loss = 1.15 (4752.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:19:20.932271: step 4390, loss = 0.89 (4237.9 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:19:21.509739: step 4400, loss = 0.95 (4088.1 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:19:22.136727: step 4410, loss = 0.91 (4452.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:22.716100: step 4420, loss = 0.98 (4496.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:23.287368: step 4430, loss = 1.00 (4737.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:19:23.859404: step 4440, loss = 0.88 (4524.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:24.429189: step 4450, loss = 1.00 (4561.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:25.010309: step 4460, loss = 1.06 (4599.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:25.590435: step 4470, loss = 0.82 (4474.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:26.152872: step 4480, loss = 1.02 (4333.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:19:26.733116: step 4490, loss = 1.02 (5026.3 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:19:27.311927: step 4500, loss = 0.94 (4389.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:27.965837: step 4510, loss = 0.99 (4667.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:19:28.542961: step 4520, loss = 0.82 (4378.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:29.121150: step 4530, loss = 0.88 (5000.7 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:19:29.702189: step 4540, loss = 0.85 (4232.6 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:19:30.258351: step 4550, loss = 0.95 (4649.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:30.828406: step 4560, loss = 1.08 (4418.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:31.398270: step 4570, loss = 1.01 (4305.3 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:19:31.966716: step 4580, loss = 0.82 (4438.4 examples/sec; 0.029 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-11 22:19:32.536339: step 4590, loss = 1.04 (4463.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:33.118420: step 4600, loss = 0.89 (4190.9 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:19:33.725212: step 4610, loss = 0.86 (4102.5 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:19:34.296137: step 4620, loss = 0.81 (4213.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:19:34.883450: step 4630, loss = 0.98 (4638.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:35.454891: step 4640, loss = 0.91 (4038.2 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:19:36.025417: step 4650, loss = 1.06 (4450.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:36.596804: step 4660, loss = 0.96 (4551.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:37.170385: step 4670, loss = 0.93 (4485.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:37.745374: step 4680, loss = 0.91 (4403.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:38.312269: step 4690, loss = 0.76 (4634.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:38.872179: step 4700, loss = 0.89 (4600.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:39.520408: step 4710, loss = 1.19 (4527.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:40.092533: step 4720, loss = 0.94 (4228.6 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:19:40.657662: step 4730, loss = 0.93 (4913.0 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:19:41.224933: step 4740, loss = 0.83 (4623.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:41.806764: step 4750, loss = 0.90 (3990.9 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:19:42.370263: step 4760, loss = 0.91 (4529.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:42.952429: step 4770, loss = 0.97 (3944.6 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:19:43.525957: step 4780, loss = 0.84 (4590.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:44.098179: step 4790, loss = 1.05 (4544.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:44.669675: step 4800, loss = 0.80 (4448.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:45.298093: step 4810, loss = 0.78 (3836.0 examples/sec; 0.033 sec/batch)\n",
      "2017-05-11 22:19:45.864722: step 4820, loss = 0.83 (4626.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:46.438055: step 4830, loss = 0.89 (4420.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:47.003446: step 4840, loss = 0.83 (5014.3 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:19:47.579285: step 4850, loss = 0.96 (4856.0 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:19:48.154464: step 4860, loss = 0.93 (4478.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:48.729751: step 4870, loss = 0.79 (4709.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:19:49.299977: step 4880, loss = 0.78 (4324.9 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:19:49.870332: step 4890, loss = 0.98 (4678.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:19:50.461010: step 4900, loss = 0.98 (4330.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:19:51.063103: step 4910, loss = 0.83 (4485.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:51.640525: step 4920, loss = 1.00 (5076.9 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:19:52.218009: step 4930, loss = 1.10 (4487.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:52.780875: step 4940, loss = 1.03 (4576.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:53.349497: step 4950, loss = 0.73 (4456.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:53.931822: step 4960, loss = 0.76 (4075.1 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:19:54.509390: step 4970, loss = 1.05 (4349.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:55.085739: step 4980, loss = 0.73 (4457.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:55.659966: step 4990, loss = 0.95 (4601.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:56.229773: step 5000, loss = 1.04 (4651.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:56.895976: step 5010, loss = 1.05 (4914.6 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:19:57.472993: step 5020, loss = 1.06 (4478.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:58.055814: step 5030, loss = 0.82 (4519.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:19:58.631514: step 5040, loss = 0.91 (4195.1 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:19:59.199671: step 5050, loss = 0.87 (4364.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:19:59.769100: step 5060, loss = 0.91 (4431.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:00.343155: step 5070, loss = 0.84 (5067.9 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:20:00.918106: step 5080, loss = 0.81 (4501.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:01.492341: step 5090, loss = 0.88 (4520.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:02.069452: step 5100, loss = 0.98 (4525.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:02.689216: step 5110, loss = 0.92 (4389.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:03.256457: step 5120, loss = 0.78 (4284.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:03.838074: step 5130, loss = 0.89 (4293.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:04.410753: step 5140, loss = 0.73 (4054.6 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:20:04.975176: step 5150, loss = 1.07 (4766.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:20:05.557830: step 5160, loss = 0.78 (4364.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:06.126250: step 5170, loss = 0.93 (4293.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:06.701257: step 5180, loss = 0.87 (4701.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:20:07.274388: step 5190, loss = 0.78 (4564.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:07.844065: step 5200, loss = 0.86 (4737.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:20:08.457742: step 5210, loss = 1.07 (4436.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:09.033426: step 5220, loss = 0.99 (4054.0 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:20:09.605241: step 5230, loss = 0.85 (4571.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:10.181581: step 5240, loss = 0.97 (4010.5 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:20:10.753731: step 5250, loss = 0.94 (4217.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:11.322479: step 5260, loss = 1.11 (4610.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:11.895958: step 5270, loss = 0.93 (4296.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:12.461916: step 5280, loss = 1.00 (4894.7 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:20:13.040441: step 5290, loss = 0.99 (4544.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:13.614485: step 5300, loss = 0.85 (4287.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:14.263707: step 5310, loss = 0.98 (4635.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:14.829608: step 5320, loss = 0.90 (4744.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:20:15.392032: step 5330, loss = 0.87 (4417.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:15.962275: step 5340, loss = 1.03 (4522.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:16.524225: step 5350, loss = 0.83 (4630.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:17.087940: step 5360, loss = 0.87 (4600.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:17.659208: step 5370, loss = 0.92 (4522.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:18.226363: step 5380, loss = 0.82 (4510.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:18.802060: step 5390, loss = 0.84 (4325.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:19.369848: step 5400, loss = 0.82 (4547.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:19.998144: step 5410, loss = 1.19 (4601.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:20.574824: step 5420, loss = 0.92 (4323.6 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:21.130508: step 5430, loss = 0.75 (4717.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:20:21.705918: step 5440, loss = 1.03 (4549.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:22.274214: step 5450, loss = 0.78 (4939.1 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:20:22.845908: step 5460, loss = 0.77 (4365.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:23.412802: step 5470, loss = 0.87 (4527.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:23.979201: step 5480, loss = 0.83 (4802.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:20:24.546737: step 5490, loss = 0.93 (4382.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:25.134362: step 5500, loss = 0.90 (4440.5 examples/sec; 0.029 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-11 22:20:25.791242: step 5510, loss = 0.98 (4330.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:26.353487: step 5520, loss = 1.00 (4336.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:26.922947: step 5530, loss = 0.74 (4316.3 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:27.488913: step 5540, loss = 0.81 (4534.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:28.060935: step 5550, loss = 0.92 (4740.1 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:20:28.622064: step 5560, loss = 0.82 (4704.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:20:29.204671: step 5570, loss = 0.84 (4195.4 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:20:29.778097: step 5580, loss = 0.81 (4295.3 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:30.359651: step 5590, loss = 0.83 (4649.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:30.930755: step 5600, loss = 0.91 (4390.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:31.571691: step 5610, loss = 0.81 (4421.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:32.138134: step 5620, loss = 0.87 (4561.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:32.718239: step 5630, loss = 0.64 (4471.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:33.290056: step 5640, loss = 0.91 (4447.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:33.866531: step 5650, loss = 0.96 (4369.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:34.431451: step 5660, loss = 0.92 (4414.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:34.994454: step 5670, loss = 0.78 (4456.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:35.556225: step 5680, loss = 0.90 (4594.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:36.132703: step 5690, loss = 0.80 (4463.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:36.706542: step 5700, loss = 0.79 (4423.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:37.322161: step 5710, loss = 0.94 (4354.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:37.887872: step 5720, loss = 0.80 (4219.6 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:38.462524: step 5730, loss = 0.83 (4246.0 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:39.033546: step 5740, loss = 0.79 (4723.1 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:20:39.607759: step 5750, loss = 0.87 (4467.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:40.188146: step 5760, loss = 0.82 (4428.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:40.763165: step 5770, loss = 0.76 (4516.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:41.339228: step 5780, loss = 0.91 (4582.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:41.906973: step 5790, loss = 0.96 (4614.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:42.490329: step 5800, loss = 0.84 (4551.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:43.098820: step 5810, loss = 1.08 (5000.5 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:20:43.663823: step 5820, loss = 0.86 (4485.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:44.234059: step 5830, loss = 0.76 (4446.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:44.803073: step 5840, loss = 0.88 (4081.4 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:20:45.373641: step 5850, loss = 0.97 (4614.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:45.938932: step 5860, loss = 0.89 (4345.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:46.506438: step 5870, loss = 0.74 (4288.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:47.080268: step 5880, loss = 0.83 (4297.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:47.647619: step 5890, loss = 0.91 (4503.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:48.210949: step 5900, loss = 0.94 (4543.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:48.844572: step 5910, loss = 0.82 (4553.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:49.433877: step 5920, loss = 0.73 (4478.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:50.003987: step 5930, loss = 0.85 (4579.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:50.577631: step 5940, loss = 0.89 (4115.2 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:20:51.146960: step 5950, loss = 0.84 (4479.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:51.722051: step 5960, loss = 0.90 (4388.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:52.289311: step 5970, loss = 0.71 (4454.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:52.863795: step 5980, loss = 1.02 (4317.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:53.450838: step 5990, loss = 0.99 (4232.6 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:54.011381: step 6000, loss = 0.68 (4453.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:54.678854: step 6010, loss = 1.16 (4387.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:55.248492: step 6020, loss = 0.96 (4698.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:20:55.822149: step 6030, loss = 0.90 (4277.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:20:56.394511: step 6040, loss = 0.71 (4731.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:20:56.977911: step 6050, loss = 1.13 (4161.8 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:20:57.543107: step 6060, loss = 0.78 (4456.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:20:58.113323: step 6070, loss = 0.73 (4132.4 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:20:58.691061: step 6080, loss = 0.91 (4535.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:20:59.260162: step 6090, loss = 1.00 (4838.2 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:20:59.828729: step 6100, loss = 0.88 (4308.2 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:21:00.466252: step 6110, loss = 0.78 (4393.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:01.043737: step 6120, loss = 0.85 (4356.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:01.607670: step 6130, loss = 0.81 (4839.0 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:21:02.174560: step 6140, loss = 0.93 (4116.1 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:21:02.747917: step 6150, loss = 0.83 (4490.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:03.314229: step 6160, loss = 0.80 (4727.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:21:03.886957: step 6170, loss = 0.88 (4705.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:21:04.445615: step 6180, loss = 0.78 (4382.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:05.020276: step 6190, loss = 0.86 (4224.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:21:05.586054: step 6200, loss = 0.92 (4441.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:06.217577: step 6210, loss = 0.87 (5811.6 examples/sec; 0.022 sec/batch)\n",
      "2017-05-11 22:21:06.782008: step 6220, loss = 0.79 (4316.9 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:21:07.350825: step 6230, loss = 0.96 (4503.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:07.913847: step 6240, loss = 0.83 (4448.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:08.472330: step 6250, loss = 0.77 (4694.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:21:09.061840: step 6260, loss = 0.89 (4466.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:09.624859: step 6270, loss = 0.91 (4644.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:10.196655: step 6280, loss = 0.84 (4381.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:10.767183: step 6290, loss = 0.79 (4788.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:21:11.337899: step 6300, loss = 0.77 (4038.2 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:21:11.963675: step 6310, loss = 0.86 (4269.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:21:12.533949: step 6320, loss = 0.95 (4597.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:13.101870: step 6330, loss = 0.84 (4459.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:13.675578: step 6340, loss = 0.88 (4998.4 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:21:14.232584: step 6350, loss = 0.87 (4615.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:14.797717: step 6360, loss = 1.15 (4511.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:15.364732: step 6370, loss = 1.00 (4654.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:15.929942: step 6380, loss = 0.90 (4393.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:16.504091: step 6390, loss = 0.74 (4509.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:17.077435: step 6400, loss = 0.81 (4485.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:17.711890: step 6410, loss = 0.91 (4469.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:18.287359: step 6420, loss = 0.89 (4366.4 examples/sec; 0.029 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-11 22:21:18.855787: step 6430, loss = 0.86 (4327.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:21:19.440230: step 6440, loss = 0.73 (4347.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:20.004287: step 6450, loss = 0.99 (4463.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:20.582309: step 6460, loss = 0.76 (4036.9 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:21:21.161133: step 6470, loss = 0.85 (4379.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:21.735613: step 6480, loss = 0.95 (4435.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:22.314111: step 6490, loss = 1.07 (4238.3 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:21:22.889324: step 6500, loss = 0.94 (4435.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:23.537371: step 6510, loss = 0.80 (4397.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:24.112167: step 6520, loss = 0.80 (4556.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:24.680272: step 6530, loss = 0.74 (5019.8 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:21:25.253552: step 6540, loss = 0.85 (4507.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:25.825421: step 6550, loss = 0.91 (4578.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:26.395147: step 6560, loss = 0.90 (4635.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:26.950569: step 6570, loss = 0.82 (4688.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:21:27.517281: step 6580, loss = 0.90 (4408.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:28.097355: step 6590, loss = 0.98 (4089.0 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:21:28.665782: step 6600, loss = 0.89 (4657.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:21:29.282117: step 6610, loss = 0.86 (4289.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:21:29.847900: step 6620, loss = 0.77 (4478.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:30.427119: step 6630, loss = 0.71 (4448.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:30.998957: step 6640, loss = 0.76 (4349.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:31.573269: step 6650, loss = 0.94 (4114.5 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:21:32.150433: step 6660, loss = 0.83 (4443.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:32.723641: step 6670, loss = 0.91 (4379.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:33.297683: step 6680, loss = 0.95 (4932.4 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:21:33.872709: step 6690, loss = 0.81 (4514.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:34.450381: step 6700, loss = 0.72 (4417.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:35.087608: step 6710, loss = 0.80 (4602.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:35.660957: step 6720, loss = 0.81 (4283.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:21:36.224946: step 6730, loss = 0.89 (4652.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:36.798275: step 6740, loss = 0.75 (4393.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:37.378803: step 6750, loss = 0.85 (4322.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:21:37.958518: step 6760, loss = 0.92 (4000.9 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:21:38.543767: step 6770, loss = 0.82 (4730.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:21:39.098060: step 6780, loss = 0.80 (4665.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:21:39.668520: step 6790, loss = 0.82 (4627.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:40.233981: step 6800, loss = 0.82 (4677.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:21:40.862442: step 6810, loss = 0.81 (4504.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:41.433317: step 6820, loss = 0.69 (4516.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:42.003496: step 6830, loss = 0.85 (4909.6 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:21:42.577363: step 6840, loss = 0.77 (4180.3 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:21:43.146709: step 6850, loss = 0.81 (4488.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:43.708181: step 6860, loss = 0.98 (5127.9 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:21:44.270838: step 6870, loss = 0.87 (4739.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:21:44.833286: step 6880, loss = 0.77 (4503.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:45.395368: step 6890, loss = 0.89 (4419.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:45.966386: step 6900, loss = 0.78 (4546.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:46.578644: step 6910, loss = 0.95 (4594.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:47.148142: step 6920, loss = 0.85 (4515.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:47.728464: step 6930, loss = 0.96 (4189.3 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:21:48.292274: step 6940, loss = 0.74 (4621.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:48.852931: step 6950, loss = 0.75 (4630.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:49.423078: step 6960, loss = 0.80 (4477.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:49.990061: step 6970, loss = 0.86 (4639.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:50.554220: step 6980, loss = 0.76 (5361.5 examples/sec; 0.024 sec/batch)\n",
      "2017-05-11 22:21:51.131779: step 6990, loss = 0.83 (4300.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:21:51.697847: step 7000, loss = 0.90 (4467.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:52.371308: step 7010, loss = 0.73 (4576.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:52.948772: step 7020, loss = 0.81 (4629.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:53.521742: step 7030, loss = 0.75 (4690.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:21:54.092161: step 7040, loss = 0.89 (4477.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:54.664198: step 7050, loss = 0.81 (4193.5 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:21:55.237095: step 7060, loss = 0.96 (4460.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:55.808757: step 7070, loss = 0.83 (4386.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:21:56.371553: step 7080, loss = 0.80 (4519.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:56.942705: step 7090, loss = 0.70 (4586.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:57.511385: step 7100, loss = 0.79 (4547.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:21:58.159151: step 7110, loss = 0.77 (4277.9 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:21:58.733704: step 7120, loss = 0.86 (4746.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:21:59.294594: step 7130, loss = 0.96 (4781.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:21:59.867823: step 7140, loss = 0.86 (4221.3 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:00.437714: step 7150, loss = 0.89 (4347.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:01.001710: step 7160, loss = 0.81 (4682.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:01.577958: step 7170, loss = 0.69 (4404.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:02.150135: step 7180, loss = 0.82 (4356.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:02.722555: step 7190, loss = 0.67 (4705.1 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:03.305802: step 7200, loss = 0.83 (4276.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:03.958310: step 7210, loss = 0.69 (3889.2 examples/sec; 0.033 sec/batch)\n",
      "2017-05-11 22:22:04.551441: step 7220, loss = 0.93 (3982.2 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:22:05.116092: step 7230, loss = 0.85 (4715.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:05.682980: step 7240, loss = 0.76 (4295.9 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:06.256038: step 7250, loss = 0.89 (4411.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:06.820067: step 7260, loss = 0.90 (4754.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:07.380519: step 7270, loss = 0.81 (4540.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:07.951487: step 7280, loss = 0.67 (4529.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:08.507039: step 7290, loss = 0.75 (4721.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:09.073299: step 7300, loss = 0.83 (4575.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:09.725819: step 7310, loss = 0.89 (4343.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:10.290462: step 7320, loss = 0.79 (4337.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:10.869531: step 7330, loss = 0.92 (4901.9 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:22:11.435774: step 7340, loss = 0.75 (4896.4 examples/sec; 0.026 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-11 22:22:11.998073: step 7350, loss = 0.69 (4417.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:12.572310: step 7360, loss = 0.84 (4634.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:13.143786: step 7370, loss = 0.71 (4446.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:13.704015: step 7380, loss = 0.94 (4751.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:14.278106: step 7390, loss = 0.86 (4303.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:14.847684: step 7400, loss = 0.84 (4543.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:15.483778: step 7410, loss = 0.84 (4192.5 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:22:16.051636: step 7420, loss = 0.76 (4564.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:16.624542: step 7430, loss = 0.94 (4937.6 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:22:17.194871: step 7440, loss = 0.63 (4369.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:17.771639: step 7450, loss = 0.90 (4391.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:18.341220: step 7460, loss = 0.77 (4387.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:18.902708: step 7470, loss = 0.76 (4483.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:19.481104: step 7480, loss = 0.90 (4354.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:20.042212: step 7490, loss = 0.95 (4570.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:20.625485: step 7500, loss = 0.89 (4317.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:21.272806: step 7510, loss = 0.67 (4547.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:21.844856: step 7520, loss = 0.82 (4636.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:22.415515: step 7530, loss = 0.66 (4398.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:22.990362: step 7540, loss = 0.94 (4375.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:23.574443: step 7550, loss = 1.07 (4047.1 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:22:24.137576: step 7560, loss = 0.67 (4722.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:24.701806: step 7570, loss = 0.84 (4553.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:25.265003: step 7580, loss = 0.95 (4654.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:25.840330: step 7590, loss = 0.76 (4764.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:26.406922: step 7600, loss = 0.82 (4160.8 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:22:27.043905: step 7610, loss = 0.78 (5083.5 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:22:27.614650: step 7620, loss = 0.70 (4333.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:28.188791: step 7630, loss = 1.01 (4230.0 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:28.752092: step 7640, loss = 1.01 (4145.9 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:22:29.326705: step 7650, loss = 0.84 (4206.3 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:29.899515: step 7660, loss = 0.76 (4591.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:30.482274: step 7670, loss = 0.75 (4240.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:31.067181: step 7680, loss = 0.67 (4037.1 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:22:31.634374: step 7690, loss = 0.73 (4424.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:32.211548: step 7700, loss = 0.80 (4368.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:32.841387: step 7710, loss = 0.70 (4413.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:33.414885: step 7720, loss = 0.93 (4289.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:33.989783: step 7730, loss = 0.91 (4641.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:34.558672: step 7740, loss = 0.76 (4493.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:35.142321: step 7750, loss = 0.86 (4185.1 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:22:35.705992: step 7760, loss = 0.82 (4751.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:36.280339: step 7770, loss = 0.71 (4505.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:36.864674: step 7780, loss = 0.74 (4658.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:37.435352: step 7790, loss = 0.87 (4343.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:38.014075: step 7800, loss = 0.83 (4505.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:38.635224: step 7810, loss = 0.91 (4629.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:39.210925: step 7820, loss = 0.75 (4553.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:39.797047: step 7830, loss = 0.97 (4961.0 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:22:40.367473: step 7840, loss = 0.89 (4283.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:40.923976: step 7850, loss = 0.73 (4771.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:41.492041: step 7860, loss = 0.73 (3995.6 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:22:42.064051: step 7870, loss = 0.87 (4575.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:42.634858: step 7880, loss = 0.93 (4441.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:43.201054: step 7890, loss = 0.77 (4705.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:43.770710: step 7900, loss = 0.87 (4412.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:44.401461: step 7910, loss = 0.90 (4713.1 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:44.979772: step 7920, loss = 0.76 (4518.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:45.545725: step 7930, loss = 0.85 (4683.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:46.104064: step 7940, loss = 0.74 (4682.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:46.661304: step 7950, loss = 0.83 (5099.3 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:22:47.236792: step 7960, loss = 0.89 (4120.3 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:22:47.809877: step 7970, loss = 0.83 (5332.4 examples/sec; 0.024 sec/batch)\n",
      "2017-05-11 22:22:48.396562: step 7980, loss = 1.01 (4223.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:48.967520: step 7990, loss = 0.74 (4803.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:49.537509: step 8000, loss = 0.67 (4756.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:50.236416: step 8010, loss = 0.72 (4473.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:50.809329: step 8020, loss = 0.76 (4607.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:51.396570: step 8030, loss = 0.77 (4192.2 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:22:51.965480: step 8040, loss = 0.80 (4739.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:52.543382: step 8050, loss = 0.74 (4476.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:53.114371: step 8060, loss = 0.61 (4457.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:53.681808: step 8070, loss = 0.95 (4377.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:54.258456: step 8080, loss = 0.58 (4752.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:54.831835: step 8090, loss = 0.75 (4631.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:22:55.403660: step 8100, loss = 0.62 (4432.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:56.018719: step 8110, loss = 0.75 (4333.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:56.599352: step 8120, loss = 0.97 (4669.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:22:57.167886: step 8130, loss = 0.89 (4874.7 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:22:57.750377: step 8140, loss = 0.74 (4308.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:58.322625: step 8150, loss = 0.64 (4308.6 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:22:58.897783: step 8160, loss = 0.77 (4404.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:22:59.481104: step 8170, loss = 0.70 (4616.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:00.058418: step 8180, loss = 0.79 (4729.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:00.630755: step 8190, loss = 0.79 (4504.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:01.209553: step 8200, loss = 1.08 (4111.5 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:23:01.828261: step 8210, loss = 0.89 (4848.1 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:23:02.401154: step 8220, loss = 0.89 (4264.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:23:02.975825: step 8230, loss = 0.95 (4488.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:03.546481: step 8240, loss = 0.81 (4382.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:04.130030: step 8250, loss = 0.73 (4222.2 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:23:04.700724: step 8260, loss = 0.66 (4584.6 examples/sec; 0.028 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-11 22:23:05.271594: step 8270, loss = 0.77 (4653.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:05.852951: step 8280, loss = 0.83 (4606.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:06.423387: step 8290, loss = 0.73 (4152.6 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:23:06.999096: step 8300, loss = 0.87 (4042.5 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:23:07.629426: step 8310, loss = 0.83 (3905.6 examples/sec; 0.033 sec/batch)\n",
      "2017-05-11 22:23:08.197606: step 8320, loss = 0.74 (4211.3 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:23:08.759340: step 8330, loss = 0.66 (4651.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:09.339502: step 8340, loss = 0.77 (4215.0 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:23:09.904625: step 8350, loss = 0.94 (4953.3 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:23:10.497652: step 8360, loss = 0.94 (4465.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:11.063760: step 8370, loss = 0.81 (4181.0 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:23:11.630842: step 8380, loss = 0.80 (4526.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:12.202350: step 8390, loss = 0.77 (4710.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:12.766681: step 8400, loss = 0.76 (4875.8 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:23:13.376863: step 8410, loss = 0.85 (4517.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:13.955661: step 8420, loss = 0.84 (4429.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:14.525620: step 8430, loss = 0.82 (4483.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:15.095927: step 8440, loss = 0.76 (4428.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:15.668456: step 8450, loss = 0.80 (4757.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:16.247761: step 8460, loss = 0.75 (4389.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:16.801314: step 8470, loss = 0.94 (4423.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:17.380146: step 8480, loss = 0.83 (4700.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:17.962124: step 8490, loss = 0.75 (4169.1 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:23:18.526157: step 8500, loss = 0.78 (4503.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:19.154536: step 8510, loss = 0.77 (4275.6 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:23:19.716623: step 8520, loss = 0.70 (4629.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:20.285151: step 8530, loss = 0.75 (4816.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:20.851590: step 8540, loss = 0.78 (4281.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:23:21.423485: step 8550, loss = 0.80 (4478.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:22.003153: step 8560, loss = 0.74 (4334.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:23:22.575561: step 8570, loss = 0.78 (4446.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:23.153525: step 8580, loss = 0.63 (4387.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:23.739976: step 8590, loss = 0.74 (4207.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:23:24.297937: step 8600, loss = 0.91 (4333.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:23:24.940018: step 8610, loss = 0.78 (4496.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:25.517136: step 8620, loss = 0.85 (4156.3 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:23:26.078216: step 8630, loss = 0.81 (4420.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:26.635517: step 8640, loss = 0.73 (4443.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:27.198864: step 8650, loss = 0.77 (4838.1 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:23:27.776308: step 8660, loss = 0.91 (4220.3 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:23:28.348937: step 8670, loss = 0.84 (4401.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:28.912210: step 8680, loss = 0.70 (4494.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:29.488877: step 8690, loss = 0.76 (4074.8 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:23:30.064132: step 8700, loss = 0.73 (4360.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:30.694124: step 8710, loss = 0.72 (4378.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:31.262495: step 8720, loss = 0.86 (4309.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:23:31.827277: step 8730, loss = 0.98 (4645.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:32.397690: step 8740, loss = 0.89 (4483.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:32.955795: step 8750, loss = 0.80 (4892.7 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:23:33.528082: step 8760, loss = 0.73 (4970.5 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:23:34.091206: step 8770, loss = 0.88 (4813.1 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:34.660026: step 8780, loss = 0.87 (4721.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:35.232776: step 8790, loss = 0.85 (4439.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:35.785036: step 8800, loss = 0.95 (4779.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:36.412700: step 8810, loss = 0.72 (4456.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:36.983386: step 8820, loss = 0.77 (4640.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:37.550173: step 8830, loss = 0.77 (4019.9 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:23:38.119613: step 8840, loss = 1.19 (4516.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:38.695557: step 8850, loss = 0.69 (4051.8 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:23:39.275452: step 8860, loss = 0.85 (4596.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:39.852493: step 8870, loss = 0.84 (4526.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:40.419603: step 8880, loss = 0.64 (4412.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:40.983526: step 8890, loss = 0.74 (4147.3 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:23:41.554990: step 8900, loss = 0.78 (4261.9 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:23:42.176566: step 8910, loss = 0.77 (4662.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:42.735230: step 8920, loss = 0.80 (4524.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:43.308550: step 8930, loss = 0.74 (4725.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:43.881516: step 8940, loss = 0.84 (4772.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:44.448786: step 8950, loss = 0.68 (4711.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:45.023383: step 8960, loss = 0.77 (4736.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:45.605495: step 8970, loss = 0.79 (4156.9 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:23:46.166099: step 8980, loss = 0.76 (4917.0 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:23:46.735262: step 8990, loss = 0.81 (4642.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:47.304726: step 9000, loss = 0.61 (4689.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:47.980112: step 9010, loss = 0.77 (4674.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:48.552516: step 9020, loss = 0.78 (4676.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:49.121444: step 9030, loss = 0.79 (4285.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:23:49.687542: step 9040, loss = 0.77 (4740.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:50.264743: step 9050, loss = 0.81 (4613.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:50.842523: step 9060, loss = 0.77 (4713.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:51.409633: step 9070, loss = 0.75 (4572.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:51.982345: step 9080, loss = 0.88 (4330.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:23:52.567818: step 9090, loss = 0.72 (4341.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:53.138915: step 9100, loss = 0.73 (4422.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:53.773814: step 9110, loss = 0.75 (4389.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:54.349233: step 9120, loss = 0.89 (4620.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:54.925856: step 9130, loss = 0.65 (4476.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:55.492889: step 9140, loss = 0.94 (4684.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:56.051584: step 9150, loss = 0.74 (4544.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:56.624196: step 9160, loss = 0.77 (4407.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:23:57.190271: step 9170, loss = 0.66 (4705.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:23:57.767620: step 9180, loss = 0.77 (4286.0 examples/sec; 0.030 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-11 22:23:58.339654: step 9190, loss = 0.90 (4583.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:23:58.915926: step 9200, loss = 0.84 (4153.9 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:23:59.557460: step 9210, loss = 0.82 (4331.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:24:00.121909: step 9220, loss = 0.83 (4876.3 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:24:00.691143: step 9230, loss = 0.81 (4826.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:01.275716: step 9240, loss = 0.72 (4415.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:01.859803: step 9250, loss = 0.77 (4512.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:02.423825: step 9260, loss = 0.79 (4834.7 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:24:02.997932: step 9270, loss = 0.68 (4653.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:03.574404: step 9280, loss = 0.85 (4629.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:04.148371: step 9290, loss = 0.68 (4738.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:04.716992: step 9300, loss = 0.87 (4364.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:05.326923: step 9310, loss = 0.85 (4774.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:05.898807: step 9320, loss = 0.65 (4529.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:06.480317: step 9330, loss = 0.61 (3983.2 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:24:07.047762: step 9340, loss = 0.87 (4476.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:07.605949: step 9350, loss = 0.80 (5130.7 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:24:08.190007: step 9360, loss = 0.81 (4337.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:24:08.765142: step 9370, loss = 0.67 (4457.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:09.329154: step 9380, loss = 0.92 (4493.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:09.895101: step 9390, loss = 0.83 (4650.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:10.468256: step 9400, loss = 0.64 (4362.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:11.095280: step 9410, loss = 0.87 (4383.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:11.666652: step 9420, loss = 0.70 (4929.2 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:24:12.237041: step 9430, loss = 0.80 (4431.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:12.797232: step 9440, loss = 0.68 (4575.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:13.375237: step 9450, loss = 0.76 (4382.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:13.942614: step 9460, loss = 0.74 (4966.2 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:24:14.518057: step 9470, loss = 0.65 (4684.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:15.087047: step 9480, loss = 0.88 (4325.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:24:15.658494: step 9490, loss = 0.80 (4535.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:16.230341: step 9500, loss = 0.82 (4160.4 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:24:16.844737: step 9510, loss = 0.86 (4517.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:17.424983: step 9520, loss = 0.72 (4192.8 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:24:18.001455: step 9530, loss = 0.76 (4639.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:18.575468: step 9540, loss = 0.77 (4356.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:19.149191: step 9550, loss = 0.75 (4785.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:19.714875: step 9560, loss = 0.83 (4746.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:20.279220: step 9570, loss = 0.67 (4238.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:24:20.857184: step 9580, loss = 0.77 (4509.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:21.426332: step 9590, loss = 0.58 (4359.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:21.995132: step 9600, loss = 0.86 (4575.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:22.596552: step 9610, loss = 0.65 (4523.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:23.166152: step 9620, loss = 0.91 (4856.2 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:24:23.735594: step 9630, loss = 0.84 (4489.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:24.308745: step 9640, loss = 0.77 (4526.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:24.883943: step 9650, loss = 0.83 (4563.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:25.458244: step 9660, loss = 0.78 (4673.1 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:26.036919: step 9670, loss = 0.81 (4391.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:26.615611: step 9680, loss = 0.55 (4299.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:24:27.188935: step 9690, loss = 0.70 (4595.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:27.764352: step 9700, loss = 0.78 (4415.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:28.407194: step 9710, loss = 0.85 (4800.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:28.987037: step 9720, loss = 0.74 (4234.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:24:29.562184: step 9730, loss = 0.74 (4013.6 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:24:30.124952: step 9740, loss = 0.72 (4669.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:30.688367: step 9750, loss = 0.71 (4691.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:31.253673: step 9760, loss = 0.80 (4669.1 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:31.827225: step 9770, loss = 0.74 (4692.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:32.408420: step 9780, loss = 0.69 (4431.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:32.975225: step 9790, loss = 0.67 (4457.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:33.539547: step 9800, loss = 0.72 (4675.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:34.172430: step 9810, loss = 0.97 (4617.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:34.741627: step 9820, loss = 0.69 (4405.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:35.308169: step 9830, loss = 0.85 (4593.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:35.876462: step 9840, loss = 0.67 (4684.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:36.443653: step 9850, loss = 0.72 (4360.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:37.016445: step 9860, loss = 0.72 (4415.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:37.589531: step 9870, loss = 0.83 (4391.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:38.157537: step 9880, loss = 1.02 (4169.9 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:24:38.715665: step 9890, loss = 0.78 (4255.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:24:39.279730: step 9900, loss = 0.79 (4507.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:39.906385: step 9910, loss = 0.81 (4938.9 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:24:40.467201: step 9920, loss = 0.66 (4818.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:41.034784: step 9930, loss = 0.69 (4411.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:41.601227: step 9940, loss = 0.73 (5075.1 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:24:42.166432: step 9950, loss = 0.61 (4322.9 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:24:42.737740: step 9960, loss = 0.72 (4530.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:43.309025: step 9970, loss = 0.80 (4338.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:24:43.881709: step 9980, loss = 0.81 (4529.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:44.449891: step 9990, loss = 0.74 (4365.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:45.016964: step 10000, loss = 0.64 (4478.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:45.695092: step 10010, loss = 0.85 (4637.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:46.257106: step 10020, loss = 0.80 (4574.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:46.835369: step 10030, loss = 0.69 (4362.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:47.403365: step 10040, loss = 0.80 (4501.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:47.977086: step 10050, loss = 0.77 (4365.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:48.545838: step 10060, loss = 0.75 (4598.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:49.119242: step 10070, loss = 0.82 (4441.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:49.689716: step 10080, loss = 0.82 (4490.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:50.256655: step 10090, loss = 0.92 (4622.5 examples/sec; 0.028 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-11 22:24:50.825903: step 10100, loss = 0.83 (4284.2 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:24:51.450390: step 10110, loss = 0.80 (4549.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:52.021302: step 10120, loss = 0.91 (4454.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:52.596363: step 10130, loss = 0.73 (4473.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:53.165487: step 10140, loss = 0.68 (4820.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:53.745282: step 10150, loss = 0.67 (3997.5 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:24:54.329625: step 10160, loss = 0.97 (4375.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:54.900574: step 10170, loss = 0.76 (4477.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:55.456532: step 10180, loss = 0.66 (4171.6 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:24:56.010188: step 10190, loss = 0.78 (4570.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:56.582527: step 10200, loss = 0.79 (4820.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:24:57.248064: step 10210, loss = 0.86 (4593.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:57.815008: step 10220, loss = 0.83 (4538.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:24:58.388193: step 10230, loss = 0.92 (4443.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:24:58.964743: step 10240, loss = 0.72 (4009.7 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:24:59.538441: step 10250, loss = 0.89 (4346.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:00.105485: step 10260, loss = 0.71 (4409.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:00.683061: step 10270, loss = 0.85 (4226.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:25:01.251846: step 10280, loss = 0.80 (4614.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:01.828804: step 10290, loss = 0.71 (4618.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:02.405710: step 10300, loss = 0.63 (4450.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:03.040480: step 10310, loss = 0.70 (4481.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:03.615273: step 10320, loss = 0.80 (4608.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:04.187905: step 10330, loss = 0.69 (4462.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:04.771285: step 10340, loss = 0.71 (4455.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:05.335869: step 10350, loss = 0.75 (4695.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:25:05.909113: step 10360, loss = 0.84 (4923.4 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:25:06.479866: step 10370, loss = 0.76 (4672.1 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:25:07.052853: step 10380, loss = 0.76 (4960.3 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:25:07.625523: step 10390, loss = 0.77 (4501.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:08.204500: step 10400, loss = 1.04 (4100.0 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:25:08.841407: step 10410, loss = 0.76 (4179.2 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:25:09.406737: step 10420, loss = 0.77 (4475.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:09.973210: step 10430, loss = 0.85 (4573.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:10.551613: step 10440, loss = 0.91 (4071.7 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:25:11.123773: step 10450, loss = 0.77 (4382.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:11.686175: step 10460, loss = 0.82 (4543.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:12.249634: step 10470, loss = 0.78 (4940.9 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:25:12.811114: step 10480, loss = 0.96 (4267.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:25:13.368401: step 10490, loss = 0.80 (4651.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:13.953891: step 10500, loss = 0.82 (3808.9 examples/sec; 0.034 sec/batch)\n",
      "2017-05-11 22:25:14.612580: step 10510, loss = 0.64 (4330.3 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:25:15.184308: step 10520, loss = 0.84 (4631.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:15.756048: step 10530, loss = 0.84 (4433.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:16.338763: step 10540, loss = 0.66 (4194.7 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:25:16.896868: step 10550, loss = 0.72 (4575.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:17.469806: step 10560, loss = 0.65 (4739.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:25:18.046835: step 10570, loss = 0.92 (4186.6 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:25:18.627939: step 10580, loss = 0.73 (3804.9 examples/sec; 0.034 sec/batch)\n",
      "2017-05-11 22:25:19.206312: step 10590, loss = 0.71 (4389.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:19.778523: step 10600, loss = 0.61 (4403.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:20.398156: step 10610, loss = 0.71 (4529.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:20.987990: step 10620, loss = 0.79 (4712.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:25:21.556528: step 10630, loss = 0.84 (4584.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:22.128598: step 10640, loss = 0.71 (4715.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:25:22.700490: step 10650, loss = 0.71 (4500.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:23.264766: step 10660, loss = 0.88 (4652.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:23.838297: step 10670, loss = 0.70 (4645.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:24.410723: step 10680, loss = 0.61 (4264.0 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:25:24.972283: step 10690, loss = 0.72 (4529.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:25.547666: step 10700, loss = 0.85 (4418.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:26.164922: step 10710, loss = 0.92 (4627.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:26.745174: step 10720, loss = 0.81 (4377.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:27.301908: step 10730, loss = 0.84 (4710.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:25:27.878844: step 10740, loss = 0.89 (4673.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:25:28.459066: step 10750, loss = 0.67 (4312.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:25:29.028290: step 10760, loss = 0.86 (4270.3 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:25:29.596834: step 10770, loss = 0.69 (4389.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:30.165843: step 10780, loss = 0.74 (4416.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:30.734821: step 10790, loss = 0.73 (4968.8 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:25:31.302005: step 10800, loss = 0.63 (4338.6 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:25:31.942250: step 10810, loss = 0.72 (4580.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:32.506616: step 10820, loss = 0.76 (4602.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:33.095038: step 10830, loss = 0.66 (4168.3 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:25:33.677082: step 10840, loss = 0.74 (4407.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:34.255399: step 10850, loss = 0.83 (4639.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:34.823117: step 10860, loss = 0.88 (4502.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:35.391855: step 10870, loss = 0.68 (4412.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:35.951840: step 10880, loss = 0.82 (4595.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:36.518150: step 10890, loss = 0.84 (4663.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:25:37.088734: step 10900, loss = 0.82 (4348.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:37.695970: step 10910, loss = 0.84 (4837.4 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:25:38.260365: step 10920, loss = 0.68 (4524.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:38.835449: step 10930, loss = 0.75 (4499.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:39.409995: step 10940, loss = 0.72 (4647.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:39.987652: step 10950, loss = 0.70 (4888.2 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:25:40.571030: step 10960, loss = 0.69 (4462.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:41.142631: step 10970, loss = 0.77 (4458.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:41.703774: step 10980, loss = 0.68 (4379.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:42.273108: step 10990, loss = 0.69 (4662.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:25:42.848050: step 11000, loss = 0.72 (4427.3 examples/sec; 0.029 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-11 22:25:43.520688: step 11010, loss = 0.76 (4294.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:25:44.088003: step 11020, loss = 0.70 (4533.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:44.662740: step 11030, loss = 0.74 (4103.0 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:25:45.223462: step 11040, loss = 0.71 (4524.7 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:45.787898: step 11050, loss = 0.77 (4595.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:46.355994: step 11060, loss = 0.85 (4771.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:25:46.937248: step 11070, loss = 0.83 (4500.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:47.511242: step 11080, loss = 0.74 (4529.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:48.076815: step 11090, loss = 0.84 (4738.1 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:25:48.643847: step 11100, loss = 0.67 (4725.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:25:49.270960: step 11110, loss = 0.81 (4188.4 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:25:49.846500: step 11120, loss = 0.70 (4331.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:25:50.411309: step 11130, loss = 0.97 (4487.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:50.988608: step 11140, loss = 0.91 (4454.7 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:51.567929: step 11150, loss = 0.75 (4293.2 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:25:52.138321: step 11160, loss = 0.69 (4705.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:25:52.711706: step 11170, loss = 0.85 (4583.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:53.286978: step 11180, loss = 0.76 (4540.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:53.867212: step 11190, loss = 0.76 (4632.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:54.429698: step 11200, loss = 0.75 (4576.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:55.078236: step 11210, loss = 0.69 (4098.8 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:25:55.653661: step 11220, loss = 0.84 (3935.0 examples/sec; 0.033 sec/batch)\n",
      "2017-05-11 22:25:56.229650: step 11230, loss = 0.83 (4122.7 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:25:56.799090: step 11240, loss = 0.70 (4183.6 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:25:57.361985: step 11250, loss = 0.80 (4501.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:57.931620: step 11260, loss = 0.70 (4519.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:25:58.511771: step 11270, loss = 0.71 (4350.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:25:59.095092: step 11280, loss = 0.84 (4172.1 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:25:59.656967: step 11290, loss = 0.68 (4780.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:00.232060: step 11300, loss = 0.73 (4496.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:00.850433: step 11310, loss = 0.76 (4296.9 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:01.415905: step 11320, loss = 0.78 (4452.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:01.990743: step 11330, loss = 0.69 (4595.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:02.569769: step 11340, loss = 0.91 (4105.1 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:26:03.156675: step 11350, loss = 0.62 (4347.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:03.720671: step 11360, loss = 0.72 (4641.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:04.304259: step 11370, loss = 0.78 (4124.7 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:26:04.862722: step 11380, loss = 0.84 (4215.9 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:05.428210: step 11390, loss = 0.78 (4421.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:06.007929: step 11400, loss = 0.73 (4295.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:06.648821: step 11410, loss = 0.74 (4861.5 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:26:07.211778: step 11420, loss = 0.90 (5023.9 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:26:07.790691: step 11430, loss = 0.94 (4559.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:08.350564: step 11440, loss = 0.73 (5506.1 examples/sec; 0.023 sec/batch)\n",
      "2017-05-11 22:26:08.927783: step 11450, loss = 0.59 (4676.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:09.489351: step 11460, loss = 0.67 (4646.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:10.061053: step 11470, loss = 0.91 (4554.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:10.623532: step 11480, loss = 0.78 (4323.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:11.190249: step 11490, loss = 0.68 (4863.4 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:26:11.749074: step 11500, loss = 0.78 (4615.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:12.373535: step 11510, loss = 0.78 (4146.4 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:26:12.944399: step 11520, loss = 0.72 (4497.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:13.528150: step 11530, loss = 0.58 (4454.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:14.099629: step 11540, loss = 0.79 (4641.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:14.684002: step 11550, loss = 0.64 (3949.2 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:26:15.258763: step 11560, loss = 0.69 (4234.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:15.825000: step 11570, loss = 0.85 (4651.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:16.391355: step 11580, loss = 0.71 (4495.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:16.973823: step 11590, loss = 0.79 (4699.2 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:17.546523: step 11600, loss = 0.85 (4475.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:18.184279: step 11610, loss = 0.70 (4441.6 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:18.751536: step 11620, loss = 0.73 (4840.7 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:26:19.334930: step 11630, loss = 0.90 (4158.3 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:26:19.896229: step 11640, loss = 0.57 (4437.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:20.472814: step 11650, loss = 0.85 (4382.5 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:21.049161: step 11660, loss = 0.67 (4240.2 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:21.609991: step 11670, loss = 0.70 (4609.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:22.182646: step 11680, loss = 0.77 (4280.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:22.745047: step 11690, loss = 0.79 (4780.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:23.331776: step 11700, loss = 0.70 (4258.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:23.977781: step 11710, loss = 0.72 (4359.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:24.548015: step 11720, loss = 0.65 (4391.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:25.114415: step 11730, loss = 0.80 (4639.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:25.677235: step 11740, loss = 0.76 (4323.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:26.257934: step 11750, loss = 0.57 (4040.7 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:26:26.816938: step 11760, loss = 0.82 (4350.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:27.385108: step 11770, loss = 0.74 (4321.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:27.935994: step 11780, loss = 0.82 (4686.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:28.509787: step 11790, loss = 0.78 (4909.7 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:26:29.078446: step 11800, loss = 1.03 (4517.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:29.694568: step 11810, loss = 0.80 (4314.5 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:30.264026: step 11820, loss = 0.75 (4275.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:30.820213: step 11830, loss = 0.80 (4904.0 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:26:31.394105: step 11840, loss = 0.78 (4588.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:31.966349: step 11850, loss = 0.77 (4608.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:32.534380: step 11860, loss = 0.79 (4598.4 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:33.096833: step 11870, loss = 0.79 (4671.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:33.654740: step 11880, loss = 0.88 (4717.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:34.225084: step 11890, loss = 0.66 (4220.8 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:34.791985: step 11900, loss = 0.62 (4366.2 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:35.437870: step 11910, loss = 0.84 (4277.7 examples/sec; 0.030 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-11 22:26:35.999336: step 11920, loss = 0.72 (4692.1 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:36.582902: step 11930, loss = 0.72 (4051.8 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:26:37.155279: step 11940, loss = 0.72 (4229.9 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:37.708444: step 11950, loss = 0.62 (4884.8 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:26:38.283583: step 11960, loss = 0.77 (4537.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:38.852526: step 11970, loss = 0.67 (4696.0 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:39.421427: step 11980, loss = 0.95 (4547.9 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:39.989054: step 11990, loss = 0.77 (4438.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:40.543586: step 12000, loss = 0.79 (4727.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:41.202956: step 12010, loss = 0.80 (4725.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:41.774231: step 12020, loss = 0.80 (4280.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:42.331576: step 12030, loss = 0.77 (4043.4 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:26:42.898442: step 12040, loss = 0.73 (4800.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:43.477371: step 12050, loss = 0.88 (4359.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:44.049641: step 12060, loss = 0.69 (4546.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:44.623273: step 12070, loss = 0.71 (4803.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:45.187083: step 12080, loss = 0.82 (4563.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:45.761577: step 12090, loss = 0.61 (4736.5 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:46.331425: step 12100, loss = 0.78 (5019.2 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:26:46.984993: step 12110, loss = 0.64 (4666.9 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:47.559119: step 12120, loss = 0.65 (5028.4 examples/sec; 0.025 sec/batch)\n",
      "2017-05-11 22:26:48.137975: step 12130, loss = 0.68 (4623.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:48.718307: step 12140, loss = 0.72 (4665.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:49.294175: step 12150, loss = 0.79 (4539.8 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:49.855857: step 12160, loss = 0.84 (4866.6 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:26:50.414565: step 12170, loss = 0.77 (4862.4 examples/sec; 0.026 sec/batch)\n",
      "2017-05-11 22:26:50.981554: step 12180, loss = 0.84 (4475.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:51.559678: step 12190, loss = 0.76 (4593.1 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:52.129693: step 12200, loss = 0.72 (4299.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:52.748551: step 12210, loss = 0.79 (4473.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:53.333919: step 12220, loss = 0.78 (4232.7 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:53.909098: step 12230, loss = 0.82 (4692.3 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:54.489865: step 12240, loss = 0.83 (4429.1 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:55.060070: step 12250, loss = 0.69 (4337.1 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:26:55.616424: step 12260, loss = 0.58 (4639.6 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:56.185462: step 12270, loss = 0.90 (4723.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:56.763681: step 12280, loss = 0.69 (4416.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:26:57.331035: step 12290, loss = 0.79 (4552.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:57.893154: step 12300, loss = 0.61 (4802.4 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:58.549980: step 12310, loss = 0.71 (4509.2 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:26:59.124573: step 12320, loss = 0.82 (4783.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:26:59.690705: step 12330, loss = 0.72 (4557.0 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:27:00.261737: step 12340, loss = 0.82 (4546.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:27:00.844726: step 12350, loss = 0.78 (4536.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:27:01.412521: step 12360, loss = 0.65 (4537.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:27:01.981033: step 12370, loss = 0.72 (4411.0 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:27:02.556276: step 12380, loss = 0.93 (4786.8 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:27:03.134109: step 12390, loss = 0.57 (4183.2 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:27:03.706869: step 12400, loss = 0.70 (4489.4 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:27:04.320837: step 12410, loss = 0.71 (4415.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:27:04.894664: step 12420, loss = 0.65 (4537.3 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:27:05.470356: step 12430, loss = 0.73 (4057.4 examples/sec; 0.032 sec/batch)\n",
      "2017-05-11 22:27:06.043796: step 12440, loss = 0.66 (4493.5 examples/sec; 0.028 sec/batch)\n",
      "2017-05-11 22:27:06.613279: step 12450, loss = 0.71 (4246.4 examples/sec; 0.030 sec/batch)\n",
      "2017-05-11 22:27:07.181815: step 12460, loss = 0.81 (4451.9 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:27:07.751628: step 12470, loss = 0.75 (4462.8 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:27:08.331086: step 12480, loss = 0.71 (4110.3 examples/sec; 0.031 sec/batch)\n",
      "2017-05-11 22:27:08.892635: step 12490, loss = 0.61 (4674.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:27:09.459010: step 12500, loss = 0.72 (4801.6 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:27:10.082043: step 12510, loss = 0.74 (4370.3 examples/sec; 0.029 sec/batch)\n",
      "2017-05-11 22:27:10.649222: step 12520, loss = 0.68 (4756.7 examples/sec; 0.027 sec/batch)\n",
      "2017-05-11 22:27:11.230751: step 12530, loss = 0.67 (4778.1 examples/sec; 0.027 sec/batch)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f22f4614929b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/kangway/tensorflow/lib/python3.5/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags_passthrough\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f22f4614929b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeleteRecursively\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f22f4614929b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m       \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kangway/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kangway/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kangway/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/kangway/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kangway/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"A binary to train CIFAR-10 using multiple GPU's with synchronous updates.\n",
    "\n",
    "Accuracy:\n",
    "cifar10_multi_gpu_train.py achieves ~86% accuracy after 100K steps (256\n",
    "epochs of data) as judged by cifar10_eval.py.\n",
    "\n",
    "Speed: With batch_size 128.\n",
    "\n",
    "System        | Step Time (sec/batch)  |     Accuracy\n",
    "--------------------------------------------------------------------\n",
    "1 Tesla K20m  | 0.35-0.60              | ~86% at 60K steps  (5 hours)\n",
    "1 Tesla K40m  | 0.25-0.35              | ~86% at 100K steps (4 hours)\n",
    "2 Tesla K20m  | 0.13-0.20              | ~84% at 30K steps  (2.5 hours)\n",
    "3 Tesla K20m  | 0.13-0.18              | ~84% at 30K steps\n",
    "4 Tesla K20m  | ~0.10                  | ~84% at 30K steps\n",
    "\n",
    "Usage:\n",
    "Please see the tutorial and website for how to download the CIFAR-10\n",
    "data set, compile the program and train the model.\n",
    "\n",
    "http://tensorflow.org/tutorials/deep_cnn/\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "import cifar10\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 1000000,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_gpus', 2,\n",
    "                            \"\"\"How many GPUs to use.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")\n",
    "\n",
    "\n",
    "def tower_loss(scope):\n",
    "  \"\"\"Calculate the total loss on a single tower running the CIFAR model.\n",
    "\n",
    "  Args:\n",
    "    scope: unique prefix string identifying the CIFAR tower, e.g. 'tower_0'\n",
    "\n",
    "  Returns:\n",
    "     Tensor of shape [] containing the total loss for a batch of data\n",
    "  \"\"\"\n",
    "  # Get images and labels for CIFAR-10.\n",
    "  images, labels = cifar10.distorted_inputs()\n",
    "\n",
    "  # Build inference Graph.\n",
    "  logits = cifar10.inference(images)\n",
    "\n",
    "  # Build the portion of the Graph calculating the losses. Note that we will\n",
    "  # assemble the total_loss using a custom function below.\n",
    "  _ = cifar10.loss(logits, labels)\n",
    "\n",
    "  # Assemble all of the losses for the current tower only.\n",
    "  losses = tf.get_collection('losses', scope)\n",
    "\n",
    "  # Calculate the total loss for the current tower.\n",
    "  total_loss = tf.add_n(losses, name='total_loss')\n",
    "\n",
    "  # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "  # same for the averaged version of the losses.\n",
    "  for l in losses + [total_loss]:\n",
    "    # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "    # session. This helps the clarity of presentation on tensorboard.\n",
    "    loss_name = re.sub('%s_[0-9]*/' % cifar10.TOWER_NAME, '', l.op.name)\n",
    "    tf.summary.scalar(loss_name, l)\n",
    "\n",
    "  return total_loss\n",
    "\n",
    "\n",
    "def average_gradients(tower_grads):\n",
    "  \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "\n",
    "  Note that this function provides a synchronization point across all towers.\n",
    "\n",
    "  Args:\n",
    "    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "      is over individual gradients. The inner list is over the gradient\n",
    "      calculation for each tower.\n",
    "  Returns:\n",
    "     List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "     across all towers.\n",
    "  \"\"\"\n",
    "  average_grads = []\n",
    "  for grad_and_vars in zip(*tower_grads):\n",
    "    # Note that each grad_and_vars looks like the following:\n",
    "    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "    grads = []\n",
    "    for g, _ in grad_and_vars:\n",
    "      # Add 0 dimension to the gradients to represent the tower.\n",
    "      expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "      # Append on a 'tower' dimension which we will average over below.\n",
    "      grads.append(expanded_g)\n",
    "\n",
    "    # Average over the 'tower' dimension.\n",
    "    grad = tf.concat(axis=0, values=grads)\n",
    "    grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "    # Keep in mind that the Variables are redundant because they are shared\n",
    "    # across towers. So .. we will just return the first tower's pointer to\n",
    "    # the Variable.\n",
    "    v = grad_and_vars[0][1]\n",
    "    grad_and_var = (grad, v)\n",
    "    average_grads.append(grad_and_var)\n",
    "  return average_grads\n",
    "\n",
    "\n",
    "def train():\n",
    "  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "    # Create a variable to count the number of train() calls. This equals the\n",
    "    # number of batches processed * FLAGS.num_gpus.\n",
    "    global_step = tf.get_variable(\n",
    "        'global_step', [],\n",
    "        initializer=tf.constant_initializer(0), trainable=False)\n",
    "\n",
    "    # Calculate the learning rate schedule.\n",
    "    num_batches_per_epoch = (cifar10.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN /\n",
    "                             FLAGS.batch_size)\n",
    "    decay_steps = int(num_batches_per_epoch * cifar10.NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    lr = tf.train.exponential_decay(cifar10.INITIAL_LEARNING_RATE,\n",
    "                                    global_step,\n",
    "                                    decay_steps,\n",
    "                                    cifar10.LEARNING_RATE_DECAY_FACTOR,\n",
    "                                    staircase=True)\n",
    "\n",
    "    # Create an optimizer that performs gradient descent.\n",
    "    opt = tf.train.GradientDescentOptimizer(lr)\n",
    "\n",
    "    # Calculate the gradients for each model tower.\n",
    "    tower_grads = []\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "      for i in xrange(FLAGS.num_gpus):\n",
    "        with tf.device('/gpu:%d' % i):\n",
    "          with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope:\n",
    "            # Calculate the loss for one tower of the CIFAR model. This function\n",
    "            # constructs the entire CIFAR model but shares the variables across\n",
    "            # all towers.\n",
    "            loss = tower_loss(scope)\n",
    "\n",
    "            # Reuse variables for the next tower.\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            # Retain the summaries from the final tower.\n",
    "            summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n",
    "\n",
    "            # Calculate the gradients for the batch of data on this CIFAR tower.\n",
    "            grads = opt.compute_gradients(loss)\n",
    "\n",
    "            # Keep track of the gradients across all towers.\n",
    "            tower_grads.append(grads)\n",
    "\n",
    "    # We must calculate the mean of each gradient. Note that this is the\n",
    "    # synchronization point across all towers.\n",
    "    grads = average_gradients(tower_grads)\n",
    "\n",
    "    # Add a summary to track the learning rate.\n",
    "    summaries.append(tf.summary.scalar('learning_rate', lr))\n",
    "\n",
    "    # Add histograms for gradients.\n",
    "    for grad, var in grads:\n",
    "      if grad is not None:\n",
    "        summaries.append(tf.summary.histogram(var.op.name + '/gradients', grad))\n",
    "\n",
    "    # Apply the gradients to adjust the shared variables.\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "      summaries.append(tf.summary.histogram(var.op.name, var))\n",
    "\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        cifar10.MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    # Group all updates to into a single train op.\n",
    "    train_op = tf.group(apply_gradient_op, variables_averages_op)\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    # Build the summary operation from the last tower summaries.\n",
    "    summary_op = tf.summary.merge(summaries)\n",
    "\n",
    "    # Build an initialization operation to run below.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start running operations on the Graph. allow_soft_placement must be set to\n",
    "    # True to build towers on GPU, as some of the ops do not have GPU\n",
    "    # implementations.\n",
    "    sess = tf.Session(config=tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=FLAGS.log_device_placement))\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the queue runners.\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)\n",
    "\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "      start_time = time.time()\n",
    "      _, loss_value = sess.run([train_op, loss])\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "\n",
    "      if step % 10 == 0:\n",
    "        num_examples_per_step = FLAGS.batch_size * FLAGS.num_gpus\n",
    "        examples_per_sec = num_examples_per_step / duration\n",
    "        sec_per_batch = duration / FLAGS.num_gpus\n",
    "\n",
    "        format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                      'sec/batch)')\n",
    "        print (format_str % (datetime.now(), step, loss_value,\n",
    "                             examples_per_sec, sec_per_batch))\n",
    "\n",
    "      if step % 100 == 0:\n",
    "        summary_str = sess.run(summary_op)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "      # Save the model checkpoint periodically.\n",
    "      if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\n",
    "        saver.save(sess, checkpoint_path, global_step=step)\n",
    "\n",
    "\n",
    "def main(argv=[\"--num_gpus=2\"]):  # pylint: disable=unused-argument\n",
    "  cifar10.maybe_download_and_extract()\n",
    "  if tf.gfile.Exists(FLAGS.train_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "  train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/cifar10_train/model.ckpt-12000\n",
      "2017-05-11 22:28:09.182949: precision @ 1 = 0.836\n"
     ]
    }
   ],
   "source": [
    "# %load cifar10_eval\n",
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Evaluation for CIFAR-10.\n",
    "\n",
    "Accuracy:\n",
    "cifar10_train.py achieves 83.0% accuracy after 100K steps (256 epochs\n",
    "of data) as judged by cifar10_eval.py.\n",
    "\n",
    "Speed:\n",
    "On a single Tesla K40, cifar10_train.py processes a single batch of 128 images\n",
    "in 0.25-0.35 sec (i.e. 350 - 600 images /sec). The model reaches ~86%\n",
    "accuracy after 100K steps in 8 hours of training time.\n",
    "\n",
    "Usage:\n",
    "Please see the tutorial and website for how to download the CIFAR-10\n",
    "data set, compile the program and train the model.\n",
    "\n",
    "http://tensorflow.org/tutorials/deep_cnn/\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import cifar10\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('eval_dir', '/tmp/cifar10_eval',\n",
    "                           \"\"\"Directory where to write event logs.\"\"\")\n",
    "tf.app.flags.DEFINE_string('eval_data', 'test',\n",
    "                           \"\"\"Either 'test' or 'train_eval'.\"\"\")\n",
    "tf.app.flags.DEFINE_string('checkpoint_dir', '/tmp/cifar10_train',\n",
    "                           \"\"\"Directory where to read model checkpoints.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('eval_interval_secs', 60 * 5,\n",
    "                            \"\"\"How often to run the eval.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_examples', 10000,\n",
    "                            \"\"\"Number of examples to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('run_once', False,\n",
    "                         \"\"\"Whether to run eval only once.\"\"\")\n",
    "\n",
    "\n",
    "def eval_once(saver, summary_writer, top_k_op, summary_op):\n",
    "  \"\"\"Run Eval once.\n",
    "\n",
    "  Args:\n",
    "    saver: Saver.\n",
    "    summary_writer: Summary writer.\n",
    "    top_k_op: Top K op.\n",
    "    summary_op: Summary op.\n",
    "  \"\"\"\n",
    "  with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "      # Restores from checkpoint\n",
    "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "      # Assuming model_checkpoint_path looks something like:\n",
    "      #   /my-favorite-path/cifar10_train/model.ckpt-0,\n",
    "      # extract global_step from it.\n",
    "      global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "    else:\n",
    "      print('No checkpoint file found')\n",
    "      return\n",
    "\n",
    "    # Start the queue runners.\n",
    "    coord = tf.train.Coordinator()\n",
    "    try:\n",
    "      threads = []\n",
    "      for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "        threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n",
    "                                         start=True))\n",
    "\n",
    "      num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))\n",
    "      true_count = 0  # Counts the number of correct predictions.\n",
    "      total_sample_count = num_iter * FLAGS.batch_size\n",
    "      step = 0\n",
    "      while step < num_iter and not coord.should_stop():\n",
    "        predictions = sess.run([top_k_op])\n",
    "        true_count += np.sum(predictions)\n",
    "        step += 1\n",
    "\n",
    "      # Compute precision @ 1.\n",
    "      precision = true_count / total_sample_count\n",
    "      print('%s: precision @ 1 = %.3f' % (datetime.now(), precision))\n",
    "\n",
    "      summary = tf.Summary()\n",
    "      summary.ParseFromString(sess.run(summary_op))\n",
    "      summary.value.add(tag='Precision @ 1', simple_value=precision)\n",
    "      summary_writer.add_summary(summary, global_step)\n",
    "    except Exception as e:  # pylint: disable=broad-except\n",
    "      coord.request_stop(e)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads, stop_grace_period_secs=10)\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "  \"\"\"Eval CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default() as g:\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    eval_data = FLAGS.eval_data == 'test'\n",
    "    images, labels = cifar10.inputs(eval_data=eval_data)\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = cifar10.inference(images)\n",
    "\n",
    "    # Calculate predictions.\n",
    "    top_k_op = tf.nn.in_top_k(logits, labels, 1)\n",
    "\n",
    "    # Restore the moving average version of the learned variables for eval.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        cifar10.MOVING_AVERAGE_DECAY)\n",
    "    variables_to_restore = variable_averages.variables_to_restore()\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.eval_dir, g)\n",
    "\n",
    "    while True:\n",
    "      eval_once(saver, summary_writer, top_k_op, summary_op)\n",
    "      if FLAGS.run_once:\n",
    "        break\n",
    "      time.sleep(FLAGS.eval_interval_secs)\n",
    "\n",
    "\n",
    "def main(argv=None):  # pylint: disable=unused-argument\n",
    "  cifar10.maybe_download_and_extract()\n",
    "  if tf.gfile.Exists(FLAGS.eval_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.eval_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.eval_dir)\n",
    "  evaluate()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ 84% accuracy at 12000 training steps (~12 minutes)\n",
    "pretty fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
